<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Event Cameara | Academic</title>
    <link>https://yixinyang-00.github.io/tag/event-cameara/</link>
      <atom:link href="https://yixinyang-00.github.io/tag/event-cameara/index.xml" rel="self" type="application/rss+xml" />
    <description>Event Cameara</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 11 Mar 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yixinyang-00.github.io/media/icon_huc70d93133c25ab28042d15d1ecec0cb0_7012_512x512_fill_lanczos_center_3.png</url>
      <title>Event Cameara</title>
      <link>https://yixinyang-00.github.io/tag/event-cameara/</link>
    </image>
    
    <item>
      <title>Learning Event Guided High Dynamic Range Video Reconstruction</title>
      <link>https://yixinyang-00.github.io/project/hdrev/</link>
      <pubDate>Sat, 11 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://yixinyang-00.github.io/project/hdrev/</guid>
      <description>&lt;html xml:lang=&#34;en&#34; lang=&#34;en&#34;&gt;
&lt;head&gt;&lt;style type=&#34;text/css&#34;&gt; * {margin:0; padding:0; text-indent:0;}
 .s1 { color: #231F20; font-family:&#34;Times New Roman&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 18pt; }
 .s2 { color: #231F20; font-family:&#34;Trebuchet MS&#34;, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 4pt; }
 .s6 { color: #231F20; font-family:&#34;Times New Roman&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 3pt; }
 .s7 { color: #231F20; font-family:&#34;Times New Roman&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .pi { color: #231F20; font-family:&#34;Times New Roman&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; margin:0pt; }
 .s8 { color: #231F20; font-family:&#34;Sitka Small&#34;; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s9 { color: #231F20; font-family:&#34;Courier New&#34;, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
&lt;/style&gt;&lt;/head&gt;
&lt;body&gt;
&lt;!-- &lt;p class=&#34;hi&#34; style=&#34;padding-left: 25pt;text-indent: 0pt;line-height: 15pt;text-align: center;&#34;&gt;Learning Event Guided High Dynamic Range Video Reconstruction&lt;/p&gt; --&gt;
&lt;p class=&#34;s2&#34; style=&#34;text-indent: 0pt;line-height:13pt;text-align: center;&#34;&gt;&lt;span class=&#34;s1&#34;&gt;Yixin Yang&lt;/span&gt;&lt;sup&gt;1,2&lt;/sup&gt;&lt;span class=&#34;s1&#34;&gt; Jin Han&lt;/span&gt;&lt;sup&gt;3,4&lt;/sup&gt;&lt;span class=&#34;s1&#34;&gt; Jinxiu Liang&lt;/span&gt;&lt;sup&gt;1,2&lt;/sup&gt;&lt;span class=&#34;s1&#34;&gt; Imari Sato&lt;/span&gt;&lt;sup&gt;3,4&lt;/sup&gt;&lt;span class=&#34;s1&#34;&gt; Boxin Shi&lt;/span&gt;&lt;sup&gt;*,1,2&lt;/sup&gt;&lt;/p&gt;
&lt;p class=&#34;s6&#34; style=&#34;text-indent: 0pt;line-height:10pt;text-align: center;&#34;&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;span class=&#34;pi&#34;&gt;National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University&lt;/span&gt;&lt;/p&gt;
&lt;p class=&#34;s6&#34; style=&#34;text-indent: 0pt;line-height:10pt;text-align: center;&#34;&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;span class=&#34;pi&#34;&gt;National Engineering Research Center of Visual Technology, School of Computer Science, Peking University&lt;/span&gt;&lt;/p&gt;
&lt;p class=&#34;s6&#34; style=&#34;text-indent: 0pt;line-height:10pt;text-align: center;&#34;&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;span class=&#34;pi&#34;&gt;Graduate School of Information Science and Technology, The University of Tokyo &lt;/span&gt;&lt;sup&gt;4&lt;/sup&gt; &lt;span class=&#34;pi&#34;&gt;National Institute of Informatics&lt;/span&gt;&lt;/p&gt;
&lt;p class=&#34;s8&#34; style=&#34;text-indent: 0pt;line-height:10pt;text-align: center;&#34;&gt;&lt;span class=&#34;s9&#34;&gt;{yangyixin93, cssherryliang, shiboxin}@pku.edu.cn {jinhan, imarik}@nii.ac.jp&lt;/span&gt;&lt;/p&gt;
&lt;/body&gt;&lt;/html&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Limited by the trade-off between frame rate and exposure time when capturing moving scenes with conventional cameras, frame based HDR video reconstruction suffers from scene-dependent exposure ratio balancing and ghosting artifacts. Event cameras provide an alternative visual representation with a much higher dynamic range and temporal resolution free from the above issues, which could be an effective guidance for HDR imaging from LDR videos. In this paper, we propose a multimodal learning framework for event guided HDR video reconstruction. In order to better leverage the knowledge of the same scene from the two modalities of visual signals, a multimodal representation alignment strategy to learn a shared latent space and a fusion module tailored to complementing two types of signals for different dynamic ranges in different regions are proposed. Temporal correlations are utilized recurrently to suppress the flickering effects in the reconstructed HDR video. The proposed HDRev-Net demonstrates state-of-the-art performance quantitatively and qualitatively for both synthetic and real-world data.&lt;/p&gt;
&lt;h2 id=&#34;animated-results&#34;&gt;Animated Results&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;1&#34;
           src=&#34;https://yixinyang-00.github.io/project/hdrev/bridge.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;2&#34;
           src=&#34;https://yixinyang-00.github.io/project/hdrev/indoor.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;3&#34;
           src=&#34;https://yixinyang-00.github.io/project/hdrev/fig_lstm.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;bibtex&#34;&gt;BibTex&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;@inproceedings{
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;yang2023HDRev,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;title={Learning Event Guided High Dynamic Range Video Reconstruction},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;author={Yixin Yang, Jin Han, Jinxiu Liang, Imari Sato, Boxin Shi},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;booktitle={Conference on Computer Vision and Pattern Recognition 2023},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;year={2023}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
