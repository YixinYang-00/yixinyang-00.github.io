<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Event Cameara | Academic</title>
    <link>https://yixinyang-00.github.io/tag/event-cameara/</link>
      <atom:link href="https://yixinyang-00.github.io/tag/event-cameara/index.xml" rel="self" type="application/rss+xml" />
    <description>Event Cameara</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 11 Mar 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yixinyang-00.github.io/media/icon_huc70d93133c25ab28042d15d1ecec0cb0_7012_512x512_fill_lanczos_center_3.png</url>
      <title>Event Cameara</title>
      <link>https://yixinyang-00.github.io/tag/event-cameara/</link>
    </image>
    
    <item>
      <title>Learning Event Guided High Dynamic Range Video Reconstruction</title>
      <link>https://yixinyang-00.github.io/project/hdrev/</link>
      <pubDate>Sat, 11 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://yixinyang-00.github.io/project/hdrev/</guid>
      <description>&lt;!DOCTYPE  html PUBLIC &#34;-//W3C//DTD XHTML 1.0 Transitional//EN&#34; &#34;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&#34;&gt;
&lt;html xmlns=&#34;http://www.w3.org/1999/xhtml&#34; xml:lang=&#34;en&#34; lang=&#34;en&#34;&gt;&lt;head&gt;&lt;meta http-equiv=&#34;Content-Type&#34; content=&#34;text/html; charset=utf-8&#34;/&gt;&lt;title&gt;CVPR23_HDRev_complete.pdf&lt;/title&gt;&lt;meta name=&#34;author&#34; content=&#34;93925&#34;/&gt;&lt;style type=&#34;text/css&#34;&gt; * {margin:0; padding:0; text-indent:0; }
 h1 { color: #231F20; font-family:&#34;Times New Roman&#34;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 .s1 { color: #231F20; font-family:&#34;Times New Roman&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s2 { color: #231F20; font-family:&#34;Trebuchet MS&#34;, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s3 { color: #231F20; font-family:&#34;Bookman Old Style&#34;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s4 { color: #231F20; font-family:&#34;Trebuchet MS&#34;, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s5 { color: #231F20; font-family:&#34;Meiryo UI&#34;, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s6 { color: #231F20; font-family:&#34;Bookman Old Style&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s7 { color: #231F20; font-family:&#34;Bookman Old Style&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .p, p { color: #231F20; font-family:&#34;Times New Roman&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; margin:0pt; }
 .s8 { color: #231F20; font-family:&#34;Sitka Small&#34;; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s9 { color: #231F20; font-family:&#34;Courier New&#34;, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s10 { color: black; font-family:&#34;Times New Roman&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s11 { color: #231F20; font-family:&#34;Times New Roman&#34;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1 style=&#34;padding-left: 25pt;text-indent: 0pt;line-height: 15pt;text-align: center;&#34;&gt;Learning Event Guided High Dynamic Range Video Reconstruction&lt;/h1&gt;&lt;p class=&#34;s2&#34; style=&#34;padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: center;&#34;&gt;&lt;span class=&#34;s1&#34;&gt;Yixin Yang&lt;/span&gt;&lt;sup&gt;1,2&lt;/sup&gt;&lt;span class=&#34;s1&#34;&gt; Jin Han&lt;/span&gt;&lt;sup&gt;3,4&lt;/sup&gt;&lt;span class=&#34;s1&#34;&gt; Jinxiu Liang&lt;/span&gt;&lt;sup&gt;1,2&lt;/sup&gt;&lt;span class=&#34;s1&#34;&gt; Imari Sato&lt;/span&gt;&lt;sup&gt;3,4&lt;/sup&gt;&lt;span class=&#34;s1&#34;&gt; Boxin Shi&lt;/span&gt;&lt;sup&gt;*,1,2&lt;/sup&gt;&lt;/p&gt;&lt;p class=&#34;s6&#34; style=&#34;padding-left: 29pt;text-indent: 0pt;text-align: center;&#34;&gt;1&lt;span class=&#34;s7&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University&lt;/span&gt;&lt;/p&gt;&lt;p class=&#34;s6&#34; style=&#34;padding-top: 3pt;padding-left: 29pt;text-indent: 0pt;text-align: center;&#34;&gt;2&lt;span class=&#34;s7&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;National Engineering Research Center of Visual Technology, School of Computer Science, Peking University&lt;/span&gt;&lt;/p&gt;&lt;p class=&#34;s6&#34; style=&#34;padding-top: 3pt;padding-left: 29pt;text-indent: 0pt;text-align: center;&#34;&gt;3&lt;span class=&#34;s7&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;Graduate School of Information Science and Technology, The University of Tokyo &lt;/span&gt;4&lt;span class=&#34;s7&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;National Institute of Informatics&lt;/span&gt;&lt;/p&gt;&lt;p class=&#34;s8&#34; style=&#34;padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: center;&#34;&gt;&lt;span class=&#34;s9&#34;&gt;{yangyixin93, cssherryliang, shiboxin}&lt;/span&gt;&lt;span class=&#34;s9&#34;&gt;@pku.edu.cn &lt;/span&gt;&lt;span class=&#34;s9&#34;&gt;{jinhan, imarik}&lt;/span&gt;&lt;span class=&#34;s9&#34;&gt;@nii.ac.jp&lt;/span&gt;&lt;/p&gt;&lt;p style=&#34;text-indent: 0pt;text-align: left;&#34;&gt;&lt;br/&gt;&lt;/p&gt;&lt;p class=&#34;s10&#34; style=&#34;padding-left: 2pt;text-indent: 0pt;text-align: left;&#34;&gt;&lt;span/&gt; &lt;span/&gt; &lt;span/&gt; &lt;span/&gt; &lt;span/&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Limited by the trade-off between frame rate and exposure time when capturing moving scenes with conventional cameras, frame based HDR video reconstruction suffers from scene-dependent exposure ratio balancing and ghosting artifacts. Event cameras provide an alternative visual representation with a much higher dynamic range and temporal resolution free from the above issues, which could be an effective guidance for HDR imaging from LDR videos. In this paper, we propose a multimodal learning framework for event guided HDR video reconstruction. In order to better leverage the knowledge of the same scene from the two modalities of visual signals, a multimodal representation alignment strategy to learn a shared latent space and a fusion module tailored to complementing two types of signals for different dynamic ranges in different regions are proposed. Temporal correlations are utilized recurrently to suppress the flickering effects in the reconstructed HDR video. The proposed HDRev-Net demonstrates state-of-the-art performance quantitatively and qualitatively for both synthetic and real-world data.&lt;/p&gt;
&lt;h2 id=&#34;animated-results&#34;&gt;Animated Results&lt;/h2&gt;
&lt;h2 id=&#34;bibtex&#34;&gt;BibTex&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;@inproceedings{
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;yang2023HDRev,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;title={Learning Event Guided High Dynamic Range Video Reconstruction},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;author={Yixin Yang, Jin Han, Jinxiu Liang, Imari Sato, Boxin Shi},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;booktitle={Conference on Computer Vision and Pattern Recognition 2023},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;year={2023}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
